{
    "recipe_id": "llm_inference_nvidia",
    "deployment_name": "t4-1b-instruct",
    "recipe_mode": "service",
    "recipe_node_pool_size": 1,
    "recipe_node_boot_volume_size_in_gbs": 200,
    "recipe_node_shape": "VM.GPU.A10.1",
    "recipe_image_uri": "iad.ocir.io/iduyx1qnmway/corrino-devops-repository:vllmv0901",
    "recipe_container_port": "8000",
    "recipe_use_shared_node_pool": true,
    "recipe_container_command_args": [
      "--model",
      "/models",
      "--tensor-parallel-size",
      "1",
      "--gpu-memory-utilization",
      "0.95",
      "--served-model-name",
      "Llama-3.2-1B-Instruct"
    ],
    "recipe_replica_count": 1,
    "recipe_nvidia_gpu_count": 1,
    "recipe_ephemeral_storage_size": 100,
    "recipe_shared_memory_volume_size_limit_in_mb": 10000,
    "input_object_storage": [
      {
        "bucket_name": "metallama321binstruct",
        "mount_location": "/models",
        "volume_size_in_gbs": 100
      }
    ]
}